{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":671256,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":508463,"modelId":523125}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Install dependencies**","metadata":{}},{"cell_type":"code","source":"%pip install -q streamlit transformers==4.46.0 protobuf==3.20.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:51:15.415475Z","iopub.execute_input":"2025-12-03T15:51:15.415849Z","iopub.status.idle":"2025-12-03T15:51:33.424673Z","shell.execute_reply.started":"2025-12-03T15:51:15.415814Z","shell.execute_reply":"2025-12-03T15:51:33.423640Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\nReason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Helpers cho Streamlit demo**","metadata":{}},{"cell_type":"code","source":"%%writefile helpers.py\nimport os\nimport pickle\nimport joblib\nimport numpy as np\nimport streamlit as st\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nMODEL_ROOT = \"/kaggle/input/toxic-comment-classification-models/tensorflow2/default/1\"\n\nML_DIR = os.path.join(MODEL_ROOT, \"ml\")\nDL_DIR = os.path.join(MODEL_ROOT, \"dl\")\n\n# BiLSTM + GRU + BiGRU model\nLSTM_MULTI_ARTIFACTS = os.path.join(DL_DIR, \"lstm_multilabel_artifacts.pkl\")\nLSTM_MULTI_MODEL     = os.path.join(DL_DIR, \"lstm_multilabel_word2vec.h5\")\n\nGRU_BIN_ARTIFACTS = os.path.join(DL_DIR, \"gru_binary_artifacts.pkl\")\nGRU_BIN_MODEL     = os.path.join(DL_DIR, \"gru_binary_word2vec.h5\")\n\nBIGRU_TOKENIZER_PATH = os.path.join(DL_DIR, \"tokenizer_bigru.pkl\")\nBIGRU_MODEL_PATH     = os.path.join(DL_DIR, \"bigru_fasttext.h5\")\nMAX_LEN_BIGRU        = 220  \n\n# RoBERTa \nROBERTA_DIR = os.path.join(DL_DIR, \"roberta_multi\")\n\n# Label names \nLABEL_COLS = [\n    \"toxic\",\n    \"severe_toxic\",\n    \"obscene\",\n    \"threat\",\n    \"insult\",\n    \"identity_hate\",\n]\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1. Text preprocessing\ndef preprocess_text(text: str) -> str:\n    \"\"\"Light text cleaning\"\"\"\n    import re\n    text = str(text).lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\n# 2. Load models\n\n# ---- ML MODELS (joblib) ----\n@st.cache_resource\ndef load_binary_ml_model(model_name: str):\n    if model_name == \"Logistic Regression\":\n        fname = \"lr_binary.joblib\"\n    elif model_name == \"SVM\":\n        fname = \"svm_binary.joblib\"\n    elif model_name == \"Naive Bayes\":\n        fname = \"nb_binary.joblib\"\n    else:\n        raise ValueError(f\"Unknown binary ML model: {model_name}\")\n\n    path = os.path.join(ML_DIR, fname)\n    model = joblib.load(path)\n    return model\n\n@st.cache_resource\ndef load_multi_ml_model(model_name: str):\n    if model_name == \"Logistic Regression\":\n        fname = \"lr_multilabel.joblib\"\n    elif model_name == \"SVM\":\n        fname = \"svm_multilabel.joblib\"\n    elif model_name == \"Naive Bayes\":\n        fname = \"nb_multilabel.joblib\"\n    else:\n        raise ValueError(f\"Unknown multi-label ML model: {model_name}\")\n\n    path = os.path.join(ML_DIR, fname)\n    model = joblib.load(path)\n    return model\n\n# ---- KERAS DEEP MODELS ----\n@st.cache_resource\ndef load_gru_binary():\n    with open(GRU_BIN_ARTIFACTS, \"rb\") as f:\n        art = pickle.load(f)\n\n    tokenizer = art.get(\"tokenizer\")\n    max_len = art.get(\"max_len\", art.get(\"maxlen\", 220))\n\n    model = tf.keras.models.load_model(GRU_BIN_MODEL, compile=False)\n    return model, tokenizer, max_len\n\n@st.cache_resource\ndef load_lstm_multilabel():\n    with open(LSTM_MULTI_ARTIFACTS, \"rb\") as f:\n        art = pickle.load(f)\n\n    tokenizer = art.get(\"tokenizer\")\n    max_len = art.get(\"max_len\", art.get(\"maxlen\", 220))\n\n    model = tf.keras.models.load_model(LSTM_MULTI_MODEL, compile=False)\n    return model, tokenizer, max_len\n\n@st.cache_resource\ndef load_bigru_multilabel():\n    with open(BIGRU_TOKENIZER_PATH, \"rb\") as f:\n        tokenizer = pickle.load(f)\n\n    model = tf.keras.models.load_model(BIGRU_MODEL_PATH, compile=False)\n    max_len = MAX_LEN_BIGRU\n    return model, tokenizer, max_len\n\n# ---- RoBERTa -----\n\n@st.cache_resource\ndef load_roberta_multilabel():\n    tokenizer = AutoTokenizer.from_pretrained(ROBERTA_DIR, local_files_only=True)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        ROBERTA_DIR,\n        local_files_only=True,\n    )\n    model.to(DEVICE)\n    model.eval()\n    return tokenizer, model\n\n# 3. Prediction helpers\ndef predict_binary(text: str, model_choice: str):\n    text_clean = preprocess_text(text)\n\n    if model_choice in [\"Logistic Regression\", \"SVM\", \"Naive Bayes\"]:\n        model = load_binary_ml_model(model_choice)\n        X = [text_clean]\n\n        if hasattr(model, \"predict_proba\"):\n            proba = model.predict_proba(X)[0]  # shape (2,) usually\n            if len(proba) == 2:\n                prob_non = float(proba[0])\n                prob_tox = float(proba[1])\n            else:\n                prob_tox = float(proba[-1])\n                prob_non = 1.0 - prob_tox\n        else:\n            scores = model.decision_function(X)\n            score = float(scores[0])\n            prob_tox = 1.0 / (1.0 + np.exp(-score))\n            prob_non = 1.0 - prob_tox\n\n    elif model_choice == \"GRU\":\n        model, tokenizer, max_len = load_gru_binary()\n        seq = tokenizer.texts_to_sequences([text_clean])\n        padded = pad_sequences(seq, maxlen=max_len)\n        prob_tox = float(model.predict(padded, verbose=0)[0][0])\n        prob_non = 1.0 - prob_tox\n\n    else:\n        raise ValueError(f\"Unknown binary model choice: {model_choice}\")\n\n    label = \"toxic\" if prob_tox >= 0.5 else \"non-toxic\"\n    return label, prob_tox, prob_non\n\n\ndef predict_multilabel(text: str, model_choice: str):\n    text_clean = preprocess_text(text)\n\n    # ----- ML models -----\n    if model_choice in [\"Logistic Regression\", \"SVM\", \"Naive Bayes\"]:\n        model = load_multi_ml_model(model_choice)\n        X = [text_clean]\n\n        if hasattr(model, \"predict_proba\"):\n            proba = model.predict_proba(X)\n\n\n            if isinstance(proba, list):\n                class_probs = [p[:, 1] for p in proba]  # positive-class prob\n                proba_matrix = np.vstack(class_probs).T  # (n_samples, n_labels)\n            else:\n                proba_matrix = proba  # (n_samples, n_labels)\n\n            probs_sample = proba_matrix[0]\n        else:\n            scores = model.decision_function(X)  # (1, n_labels)\n            probs_sample = 1.0 / (1.0 + np.exp(-scores[0]))\n\n        probs = {label: float(p) for label, p in zip(LABEL_COLS, probs_sample)}\n\n    # ----- LSTM / BiLSTM -----\n    elif model_choice == \"BiLSTM\":\n        model, tokenizer, max_len = load_lstm_multilabel()\n        seq = tokenizer.texts_to_sequences([text_clean])\n        padded = pad_sequences(seq, maxlen=max_len)\n        probs_sample = model.predict(padded, verbose=0)[0]  # (6,)\n        probs = {label: float(p) for label, p in zip(LABEL_COLS, probs_sample)}\n\n    # ----- BiGRU -----\n    elif model_choice == \"BiGRU\":\n        model, tokenizer, max_len = load_bigru_multilabel()\n        seq = tokenizer.texts_to_sequences([text_clean])\n        padded = pad_sequences(seq, maxlen=max_len)\n        probs_sample = model.predict(padded, verbose=0)[0]  # (6,)\n        probs = {label: float(p) for label, p in zip(LABEL_COLS, probs_sample)}\n\n    # ----- RoBERTa -----\n    elif model_choice == \"RoBERTa\":\n        tokenizer, model = load_roberta_multilabel()\n        enc = tokenizer(\n            text_clean,\n            add_special_tokens=True,\n            truncation=True,\n            max_length=220,  # same as training\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        input_ids = enc[\"input_ids\"].to(DEVICE)\n        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            probs_tensor = torch.sigmoid(logits).cpu().numpy()[0]\n\n        probs = {label: float(p) for label, p in zip(LABEL_COLS, probs_tensor)}\n\n    else:\n        raise ValueError(f\"Unknown multi-label model choice: {model_choice}\")\n\n    return probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:51:33.463966Z","iopub.execute_input":"2025-12-03T15:51:33.464291Z","iopub.status.idle":"2025-12-03T15:51:33.486849Z","shell.execute_reply.started":"2025-12-03T15:51:33.464268Z","shell.execute_reply":"2025-12-03T15:51:33.485773Z"}},"outputs":[{"name":"stdout","text":"Overwriting helpers.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **Streamlit app.py**","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nfrom helpers import LABEL_COLS, predict_binary, predict_multilabel\n\nst.set_page_config(page_title=\"Toxic Comment Classifier\", layout=\"wide\")\n\nst.title(\"ğŸ§ª Toxic Comment Classification Demo\")\n\ntab_binary, tab_multi = st.tabs([\"Binary Classification\", \"Multi-label Classification\"])\n\n# 1. Binary\nwith tab_binary:\n    st.subheader(\"Binary Classification (toxic vs non-toxic)\")\n\n    binary_model_choice = st.selectbox(\n        \"Choose a model\",\n        [\"Logistic Regression\", \"SVM\", \"Naive Bayes\", \"GRU\"],\n        key=\"binary_model_choice\",\n    )\n\n    user_input_bin = st.text_area(\n        \"Enter a comment\",\n        height=150,\n        key=\"binary_input\",\n    )\n\n    if st.button(\"Predict (Binary)\", key=\"predict_binary_btn\"):\n        if not user_input_bin.strip():\n            st.warning(\"Please enter some text before predicting.\")\n        else:\n            label, prob_tox, prob_non = predict_binary(user_input_bin, binary_model_choice)\n\n            st.markdown(f\"**Predicted label:** `{label}`\")\n            st.write(\"**Confidence:**\")\n            st.write(f\"- Toxic: `{prob_tox:.4f}`\")\n            st.write(f\"- Non-toxic: `{prob_non:.4f}`\")\n\n# 2. Multi-label\nwith tab_multi:\n    st.subheader(\"Multi-label Classification (6 toxicity categories)\")\n\n    multi_model_choice = st.selectbox(\n        \"Choose a model\",\n        [\"Logistic Regression\", \"SVM\", \"Naive Bayes\", \"BiLSTM\", \"BiGRU\", \"RoBERTa\"],\n        key=\"multi_model_choice\",\n    )\n\n    user_input_multi = st.text_area(\n        \"Enter a comment\",\n        height=150,\n        key=\"multi_input\",\n    )\n\n    if st.button(\"Predict (Multi-label)\", key=\"predict_multi_btn\"):\n        if not user_input_multi.strip():\n            st.warning(\"Please enter some text before predicting.\")\n        else:\n            probs = predict_multilabel(user_input_multi, multi_model_choice)\n\n            st.write(\"**Predicted probabilities:**\")\n            for label in LABEL_COLS:\n                st.write(f\"- {label}: `{probs[label]:.4f}`\")\n\n            st.write(\"---\")\n            st.write(\"**Labels with probability â‰¥ 0.5:**\")\n            active_labels = [lbl for lbl, p in probs.items() if p >= 0.5]\n            if active_labels:\n                st.write(\", \".join(active_labels))\n            else:\n                st.write(\"_None (all below threshold 0.5)_\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:51:35.266110Z","iopub.execute_input":"2025-12-03T15:51:35.266463Z","iopub.status.idle":"2025-12-03T15:51:35.273365Z","shell.execute_reply.started":"2025-12-03T15:51:35.266438Z","shell.execute_reply":"2025-12-03T15:51:35.272454Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%pip install pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:51:04.080800Z","iopub.execute_input":"2025-12-03T15:51:04.081130Z","iopub.status.idle":"2025-12-03T15:51:09.460975Z","shell.execute_reply.started":"2025-12-03T15:51:04.081105Z","shell.execute_reply":"2025-12-03T15:51:09.460076Z"}},"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.3)\nDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pyngrok import ngrok\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nNGROK_AUTH_TOKEN = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n\nif NGROK_AUTH_TOKEN is None or NGROK_AUTH_TOKEN.strip() == \"\":\n    raise ValueError(\n        \"NGROK_AUTH_TOKEN secret is empty or not set. \"\n        \"Go to Add-ons â†’ Secrets and create a secret named 'NGROK_AUTH_TOKEN' with your token.\"\n    )\n\n# Set token for pyngrok\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Kill any old tunnels\nngrok.kill()\n\n# Open tunnel to port 8501\npublic_url = ngrok.connect(8501, \"http\")\nprint(\"ğŸ”— Public URL:\", public_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:57:55.238338Z","iopub.execute_input":"2025-12-03T15:57:55.238696Z","iopub.status.idle":"2025-12-03T15:57:55.757097Z","shell.execute_reply.started":"2025-12-03T15:57:55.238673Z","shell.execute_reply":"2025-12-03T15:57:55.756219Z"}},"outputs":[{"name":"stdout","text":"ğŸ”— Public URL: NgrokTunnel: \"https://unvillainous-subobscurely-larae.ngrok-free.dev\" -> \"http://localhost:8501\"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!streamlit run app.py --server.port 8501 --server.address 0.0.0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:58:09.006660Z","iopub.execute_input":"2025-12-03T15:58:09.006971Z"}},"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  URL: \u001b[0m\u001b[1mhttp://0.0.0.0:8501\u001b[0m\n\u001b[0m\n2025-12-03 15:58:38.478634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764777518.724040     142 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764777518.810336     142 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-12-03 16:00:13.972442: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":null}]}